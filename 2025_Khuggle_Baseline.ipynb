{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangjun315/FedAvg-Pytorch/blob/main/2025_Khuggle_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diffusion Distillation Challenge Baseline**\n",
        "\n",
        "**Teacher**: DDPM-style UNet on Tiny-ImageNet (64√ó64)  \n",
        "**Student**: Smaller DDPM-style UNet\n",
        "\n",
        "Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùÄ Îã§ÏùåÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.\n",
        "\n",
        "1. Tiny-ImageNet Îç∞Ïù¥ÌÑ∞ Î°úÎçî / val ÌîåÎû´Îãù\n",
        "2. DDPM Teacher UNet Ï†ïÏùò\n",
        "3. TeacherÏö© DDIM ÏÉòÌîåÎü¨ + FID Í≥ÑÏÇ∞ (pytorch-fid ÏÇ¨Ïö©)\n",
        "4. Student DDPM UNet (Íµ¨Ï°∞ Í≥†Ï†ï)\n",
        "5. Teacher ‚Üí Student epsilon distillation ÌïôÏäµ ÏΩîÎìú (baseline)\n",
        "6. Student ÏÉòÌîå Î∞è FID ÏòàÏãú Í≥ÑÏÇ∞\n",
        "7. (ÎåÄÌöåÏö©) Ï†êÏàò Í≥ÑÏÇ∞ ÏòàÏãú\n",
        "\n",
        "================================================================================\n",
        "1. Tiny-ImageNet data loader / val set flattening\n",
        "2. Definition of the DDPM Teacher UNet\n",
        "3. DDIM sampler for the Teacher + FID computation (using pytorch-fid)\n",
        "4. Student DDPM UNet (architecture fixed)\n",
        "5. Training code for Teacher ‚Üí Student epsilon distillation (baseline)\n",
        "6. Example of Student sampling and FID computation\n",
        "7. (For the competition) Example of score calculation"
      ],
      "metadata": {
        "id": "ar8twX1Aqvyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. TinyImageNet Download and Unzip**\n"
      ],
      "metadata": {
        "id": "lMZPm-8jjxKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tiny-ImageNet Îã§Ïö¥Î°úÎìú & ÏïïÏ∂ïÌï¥Ï†ú\n",
        "\n",
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip -O tiny-imagenet-200.zip\n",
        "\n",
        "!unzip -q tiny-imagenet-200.zip -d .\n",
        "\n",
        "!ls tiny-imagenet-200\n",
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKELrler-xmW",
        "outputId": "780918e7-d8d0-435b-cfc4-e73344764416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-23 08:17:38--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs231n.stanford.edu/tiny-imagenet-200.zip [following]\n",
            "--2025-11-23 08:17:38--  https://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‚Äòtiny-imagenet-200.zip‚Äô\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  5.32MB/s    in 30s     \n",
            "\n",
            "2025-11-23 08:18:08 (7.98 MB/s) - ‚Äòtiny-imagenet-200.zip‚Äô saved [248100043/248100043]\n",
            "\n",
            "test  train  val  wnids.txt  words.txt\n",
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pytorch-fid) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pytorch-fid) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pytorch-fid) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from pytorch-fid) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from pytorch-fid) (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.1->pytorch-fid) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid) (3.0.3)\n",
            "Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytorch-fid\n",
            "Successfully installed pytorch-fid-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Set up the environments and import the basic module**\n"
      ],
      "metadata": {
        "id": "RmpFbHUDj7WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ÌôòÍ≤Ω ÏÑ§Ï†ï & Í∏∞Î≥∏ import\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import sys\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[info] using device:\", device)\n",
        "\n",
        "def format_time(secs: float) -> str:\n",
        "    secs = int(secs)\n",
        "    h = secs // 3600\n",
        "    m = (secs % 3600) // 60\n",
        "    s = secs % 60\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "\n",
        "def make_beta_schedule(num_train_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
        "    betas = torch.linspace(beta_start, beta_end, num_train_timesteps, dtype=torch.float32)\n",
        "    alphas = 1.0 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "    return betas, alphas, alphas_cumprod\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1DJF0JN2DHC",
        "outputId": "c6f6ad8c-4d27-405a-8878-b3fbf316d2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch: 2.9.0+cu126\n",
            "[info] using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Dataset loader**"
      ],
      "metadata": {
        "id": "Snul7tAouTs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "def get_tiny_train_loader(\n",
        "    data_root: str,\n",
        "    batch_size: int = 128,\n",
        "    num_workers: int = 4,\n",
        "):\n",
        "    \"\"\"\n",
        "    Tiny-ImageNet train Ìè¥Îçî Í∏∞Ï§Ä Î°úÎçî.\n",
        "    - data_root/tiny-imagenet-200/train/<class>/*.JPEG\n",
        "    - ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÎäî 64x64Î°ú resize, [-1,1]Î°ú normalize.\n",
        "    \"\"\"\n",
        "    train_dir = os.path.join(data_root, \"tiny-imagenet-200\", \"train\")\n",
        "    print(\"[info] train_dir:\", train_dir)\n",
        "\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),  # [0,1]\n",
        "        transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                             (0.5, 0.5, 0.5)),  # [-1,1]\n",
        "    ])\n",
        "\n",
        "    dataset = ImageFolder(train_dir, transform=tfm)\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    print(\"[info] Tiny-ImageNet train size:\", len(dataset))\n",
        "    return loader\n",
        "\n",
        "\n",
        "def flatten_tiny_val(\n",
        "    data_root: str,\n",
        "    out_dir: str = \"./tiny_val_flat\",\n",
        "):\n",
        "    \"\"\"\n",
        "\n",
        "    - data_root/tiny-imagenet-200/val/images/*.JPEG\n",
        "    - out_dir/val_000000.jpeg ...\n",
        "    \"\"\"\n",
        "    root_tiny = Path(data_root) / \"tiny-imagenet-200\"\n",
        "    val_dir = root_tiny / \"val\" / \"images\"\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    cnt = 0\n",
        "    if val_dir.exists():\n",
        "        for img_path in val_dir.glob(\"*.JPEG\"):\n",
        "            dst = out_dir / f\"val_{cnt:06d}.jpeg\"\n",
        "            if not dst.exists():\n",
        "                shutil.copy(str(img_path), str(dst))\n",
        "            cnt += 1\n",
        "        print(\"flattened val images:\", cnt, \"->\", out_dir)\n",
        "    else:\n",
        "        print(\"WARNING: val/images not found under\", root_tiny)\n",
        "\n",
        "    return str(out_dir)\n",
        "\n",
        "flatten_tiny_val(\n",
        "    data_root=\"/content/\",  # Tiny-ImageNet ÏïïÏ∂ïÏùÑ ÌíÄÏñ¥Îëî Î£®Ìä∏\n",
        "    out_dir=\"./tiny_val_flat\",\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "o6Vr1Vr149s2",
        "outputId": "bb56532e-5e31-4035-bfe1-a2f6335940db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flattened val images: 10000 -> tiny_val_flat\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tiny_val_flat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Sinusoidal time embedding & UNet Blocks**"
      ],
      "metadata": {
        "id": "EoyZT98vuvYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, t):\n",
        "        \"\"\"\n",
        "        t: (B,) long or float, time index [0..T-1]\n",
        "        return: (B, dim)\n",
        "        \"\"\"\n",
        "        device = t.device\n",
        "        half = self.dim // 2\n",
        "        freqs = torch.exp(\n",
        "            torch.arange(half, device=device, dtype=torch.float32)\n",
        "            * -(math.log(10000.0) / (half - 1))\n",
        "        )\n",
        "        if t.dtype != torch.float32:\n",
        "            t = t.float()\n",
        "        args = t[:, None] * freqs[None, :]\n",
        "        emb = torch.cat([args.sin(), args.cos()], dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_dim, groups=8):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.GroupNorm(groups, in_ch)\n",
        "        self.act1  = nn.SiLU()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_dim, out_ch)\n",
        "        )\n",
        "\n",
        "        self.norm2 = nn.GroupNorm(groups, out_ch)\n",
        "        self.act2  = nn.SiLU()\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "\n",
        "        if in_ch != out_ch:\n",
        "            self.skip = nn.Conv2d(in_ch, out_ch, 1)\n",
        "        else:\n",
        "            self.skip = nn.Identity()\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        h = self.conv1(self.act1(self.norm1(x)))\n",
        "        t_h = self.time_mlp(t_emb)[:, :, None, None]\n",
        "        h = h + t_h\n",
        "        h = self.conv2(self.act2(self.norm2(h)))\n",
        "        return h + self.skip(x)\n",
        "\n",
        "\n",
        "class DownBlockT(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_dim):\n",
        "        super().__init__()\n",
        "        self.res1 = ResBlock(in_ch, out_ch, time_dim)\n",
        "        self.res2 = ResBlock(out_ch, out_ch, time_dim)\n",
        "        self.down = nn.Conv2d(out_ch, out_ch, 3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        x = self.res1(x, t_emb)\n",
        "        x = self.res2(x, t_emb)\n",
        "        skip = x\n",
        "        x = self.down(x)\n",
        "        return x, skip\n",
        "\n",
        "\n",
        "class UpBlockT(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_dim):\n",
        "        super().__init__()\n",
        "        self.res1 = ResBlock(in_ch, out_ch, time_dim)\n",
        "        self.res2 = ResBlock(out_ch, out_ch, time_dim)\n",
        "        self.up   = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
        "        self.conv = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "\n",
        "    def forward(self, x, skip, t_emb):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.res1(x, t_emb)\n",
        "        x = self.res2(x, t_emb)\n",
        "        x = self.conv(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zrkwlZI649qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Teacher model structure**"
      ],
      "metadata": {
        "id": "p7ryqbtIu4td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TeacherUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Tiny-ImageNet 64x64Ïö© DDPM-style Teacher UNet (epsilon prediction).\n",
        "    ÏûÖÎ†•: x_t (B,3,64,64), t (B,) int64\n",
        "    Ï∂úÎ†•: eps_pred (B,3,64,64)\n",
        "    \"\"\"\n",
        "    def __init__(self, img_ch=3, base_ch=128, time_dim=512):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPosEmb(time_dim),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "        )\n",
        "\n",
        "        self.inc = nn.Conv2d(img_ch, base_ch, 3, padding=1)\n",
        "\n",
        "        self.down1 = DownBlockT(base_ch,      base_ch*2, time_dim)  # 64->32\n",
        "        self.down2 = DownBlockT(base_ch*2,    base_ch*4, time_dim)  # 32->16\n",
        "        self.down3 = DownBlockT(base_ch*4,    base_ch*4, time_dim)  # 16->8\n",
        "        self.down4 = DownBlockT(base_ch*4,    base_ch*4, time_dim)  # 8->4\n",
        "\n",
        "        self.mid1 = ResBlock(base_ch*4, base_ch*4, time_dim)\n",
        "        self.mid2 = ResBlock(base_ch*4, base_ch*4, time_dim)\n",
        "\n",
        "        self.up4 = UpBlockT(base_ch*4 + base_ch*4, base_ch*4, time_dim)  # 4->8\n",
        "        self.up3 = UpBlockT(base_ch*4 + base_ch*4, base_ch*4, time_dim)  # 8->16\n",
        "        self.up2 = UpBlockT(base_ch*4 + base_ch*4, base_ch*2, time_dim)  # 16->32\n",
        "        self.up1 = UpBlockT(base_ch*2 + base_ch*2, base_ch,   time_dim)  # 32->64\n",
        "\n",
        "        self.outc = nn.Conv2d(base_ch, img_ch, 3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t_emb = self.time_mlp(t)\n",
        "        x0 = self.inc(x)\n",
        "        x1, s1 = self.down1(x0, t_emb)\n",
        "        x2, s2 = self.down2(x1, t_emb)\n",
        "        x3, s3 = self.down3(x2, t_emb)\n",
        "        x4, s4 = self.down4(x3, t_emb)\n",
        "\n",
        "        m  = self.mid1(x4, t_emb)\n",
        "        m  = self.mid2(m,  t_emb)\n",
        "\n",
        "        u4 = self.up4(m,  s4, t_emb)\n",
        "        u3 = self.up3(u4, s3, t_emb)\n",
        "        u2 = self.up2(u3, s2, t_emb)\n",
        "        u1 = self.up1(u2, s1, t_emb)\n",
        "\n",
        "        out = self.outc(u1)\n",
        "        return out  # eps prediction\n"
      ],
      "metadata": {
        "id": "jMcNxaOSu33H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. DDIM Sampler**"
      ],
      "metadata": {
        "id": "vYUp1tuGvKeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def ddim_sample(model, alphas_cumprod, num_train_timesteps, z, steps=50):\n",
        "    \"\"\"\n",
        "    DDIM(eta=0) deterministic sampler.\n",
        "    - model: epsilon prediction UNet (Teacher or Student)\n",
        "    - alphas_cumprod: (T,)\n",
        "    - z: (B,3,64,64) ~ N(0,I)\n",
        "    \"\"\"\n",
        "    device = z.device\n",
        "    b = z.size(0)\n",
        "    x = z\n",
        "\n",
        "    T = num_train_timesteps\n",
        "    step_indices = torch.linspace(T - 1, 0, steps=steps, device=device).long()\n",
        "\n",
        "    for i, t in enumerate(step_indices):\n",
        "        t_batch = t.repeat(b)\n",
        "        eps = model(x, t_batch)\n",
        "\n",
        "        alpha_t = alphas_cumprod[t]\n",
        "        sqrt_alpha_t = alpha_t.sqrt()\n",
        "        sqrt_one_minus_alpha_t = (1.0 - alpha_t).sqrt()\n",
        "\n",
        "        x0_pred = (x - sqrt_one_minus_alpha_t * eps) / sqrt_alpha_t\n",
        "        x0_pred = x0_pred.clamp(-1.0, 1.0)\n",
        "\n",
        "        if i == steps - 1:\n",
        "            x = x0_pred\n",
        "        else:\n",
        "            t_next = step_indices[i + 1]\n",
        "            alpha_next = alphas_cumprod[t_next]\n",
        "            sqrt_alpha_next = alpha_next.sqrt()\n",
        "            sqrt_one_minus_alpha_next = (1.0 - alpha_next).sqrt()\n",
        "            x = sqrt_alpha_next * x0_pred + sqrt_one_minus_alpha_next * eps\n",
        "\n",
        "    return x  # [-1,1] approx x0\n"
      ],
      "metadata": {
        "id": "3akSQQxAvJ1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Student model structure**"
      ],
      "metadata": {
        "id": "c5jOF2vKu_69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StudentUNetDDPM(nn.Module):\n",
        "    \"\"\"\n",
        "    ÌïôÏÉù Î™®Îç∏: TeacherÏôÄ Í∞ôÏùÄ Íµ¨Ï°∞ÏßÄÎßå base_chÎ•º Ï§ÑÏù∏ DDPM-style UNet.\n",
        "    base_ch=64 (TeacherÎäî 128)\n",
        "    ÏûÖÎ†•: x_t (B,3,64,64), t (B,)\n",
        "    Ï∂úÎ†•: eps_pred (B,3,64,64)\n",
        "    \"\"\"\n",
        "    def __init__(self, img_ch=3, base_ch=64, time_dim=512):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPosEmb(time_dim),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "        )\n",
        "\n",
        "        self.inc = nn.Conv2d(img_ch, base_ch, 3, padding=1)\n",
        "\n",
        "        self.down1 = DownBlockT(base_ch,      base_ch*2, time_dim)  # 64->32\n",
        "        self.down2 = DownBlockT(base_ch*2,    base_ch*4, time_dim)  # 32->16\n",
        "        self.down3 = DownBlockT(base_ch*4,    base_ch*4, time_dim)  # 16->8\n",
        "        self.down4 = DownBlockT(base_ch*4,    base_ch*4, time_dim)  # 8->4\n",
        "\n",
        "        self.mid1 = ResBlock(base_ch*4, base_ch*4, time_dim)\n",
        "        self.mid2 = ResBlock(base_ch*4, base_ch*4, time_dim)\n",
        "\n",
        "        self.up4 = UpBlockT(base_ch*4 + base_ch*4, base_ch*4, time_dim)  # 4->8\n",
        "        self.up3 = UpBlockT(base_ch*4 + base_ch*4, base_ch*4, time_dim)  # 8->16\n",
        "        self.up2 = UpBlockT(base_ch*4 + base_ch*4, base_ch*2, time_dim)  # 16->32\n",
        "        self.up1 = UpBlockT(base_ch*2 + base_ch*2, base_ch,   time_dim)  # 32->64\n",
        "\n",
        "        self.outc = nn.Conv2d(base_ch, img_ch, 3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t_emb = self.time_mlp(t)\n",
        "        x0 = self.inc(x)\n",
        "        x1, s1 = self.down1(x0, t_emb)\n",
        "        x2, s2 = self.down2(x1, t_emb)\n",
        "        x3, s3 = self.down3(x2, t_emb)\n",
        "        x4, s4 = self.down4(x3, t_emb)\n",
        "\n",
        "        m  = self.mid1(x4, t_emb)\n",
        "        m  = self.mid2(m,  t_emb)\n",
        "\n",
        "        u4 = self.up4(m,  s4, t_emb)\n",
        "        u3 = self.up3(u4, s3, t_emb)\n",
        "        u2 = self.up2(u3, s2, t_emb)\n",
        "        u1 = self.up1(u2, s1, t_emb)\n",
        "\n",
        "        out = self.outc(u1)\n",
        "        return out  # eps prediction\n"
      ],
      "metadata": {
        "id": "jgkLNcF5uQUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Teacher epsilon Wrapper + Student Distillation training loop**"
      ],
      "metadata": {
        "id": "IhwWGGZOvYlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TeacherEpsWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    teacher_tiny.ptÏóêÏÑú TeacherUNetÍ≥º alphas_cumprodÎ•º Î°úÎìúÌïòÎäî ÎûòÌçº.\n",
        "    - forward(x_t, t) -> eps_teacher\n",
        "    \"\"\"\n",
        "    def __init__(self, ckpt_path: str, device=\"cuda\"):\n",
        "        super().__init__()\n",
        "        ckpt = torch.load(ckpt_path, map_location=device)\n",
        "        self.model = TeacherUNet().to(device)\n",
        "        self.model.load_state_dict(ckpt[\"model\"])\n",
        "        self.model.eval()\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "        self.alphas_cumprod = ckpt[\"alphas_cumprod\"].to(device)  # (T,)\n",
        "        self.num_train_timesteps = ckpt[\"num_train_timesteps\"]\n",
        "        self.device = device\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x_t, t):\n",
        "        return self.model(x_t, t)\n",
        "\n",
        "\n",
        "def train_student_ddpm_distill(\n",
        "    data_root: str,\n",
        "    teacher_ckpt: str = \"teacher_tiny.pt\",\n",
        "    epochs: int = 20,\n",
        "    batch_size: int = 128,\n",
        "    lr: float = 2e-4,\n",
        "    ema_decay: float = 0.999,\n",
        "    save_path: str = \"student_ddpm_tiny.pt\",\n",
        "):\n",
        "    \"\"\"\n",
        "    DDPM Teacher ‚Üí DDPM Student epsilon distillation (baseline).\n",
        "    - Tiny-ImageNet trainÏóêÏÑú x0Î•º ÎΩëÍ≥†\n",
        "    - q(x_t | x0)Î°ú x_tÎ•º ÏÉòÌîåÎßÅ\n",
        "    - Teacher/StudentÏùò epsilonÏùÑ L2Î°ú ÎßûÏ∂§\n",
        "    \"\"\"\n",
        "    train_loader = get_tiny_train_loader(data_root, batch_size)\n",
        "\n",
        "    teacher = TeacherEpsWrapper(ckpt_path=teacher_ckpt, device=device)\n",
        "    alphas_cumprod = teacher.alphas_cumprod  # (T,)\n",
        "    T = teacher.num_train_timesteps\n",
        "\n",
        "    student = StudentUNetDDPM().to(device)\n",
        "    ema = StudentUNetDDPM().to(device)\n",
        "    ema.load_state_dict(student.state_dict())\n",
        "    for p in ema.parameters():\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "    opt = optim.AdamW(student.parameters(), lr=lr)\n",
        "    mse = nn.MSELoss()\n",
        "\n",
        "    total_iters = epochs * len(train_loader)\n",
        "    global_step = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"[student distill] start | epochs={epochs}, total_iters‚âà{total_iters}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for it_in_epoch, (x0, _) in enumerate(train_loader, start=1):\n",
        "            global_step += 1\n",
        "            student.train()\n",
        "\n",
        "            x0 = x0.to(device)  # [-1,1]\n",
        "            b = x0.size(0)\n",
        "\n",
        "            t = torch.randint(low=0, high=T, size=(b,), device=device, dtype=torch.long)\n",
        "            eps = torch.randn_like(x0)\n",
        "\n",
        "            alpha_t = alphas_cumprod[t].view(b, 1, 1, 1)\n",
        "            sqrt_alpha_t = alpha_t.sqrt()\n",
        "            sqrt_one_minus_alpha_t = (1.0 - alpha_t).sqrt()\n",
        "            x_t = sqrt_alpha_t * x0 + sqrt_one_minus_alpha_t * eps\n",
        "\n",
        "            with torch.no_grad():\n",
        "                eps_teacher = teacher(x_t, t)\n",
        "\n",
        "            eps_student = student(x_t, t)\n",
        "            loss = mse(eps_student, eps_teacher)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            # EMA update\n",
        "            with torch.no_grad():\n",
        "                for p_ema, p in zip(ema.parameters(), student.parameters()):\n",
        "                    p_ema.data.mul_(ema_decay).add_(p.data, alpha=1.0 - ema_decay)\n",
        "\n",
        "            if global_step % 50 == 0 or global_step == 1:\n",
        "                elapsed = time.time() - start_time\n",
        "                progress = global_step / max(total_iters, 1)\n",
        "                eta = elapsed / max(progress, 1e-8) - elapsed\n",
        "                print(\n",
        "                    f\"[student distill] epoch {epoch+1}/{epochs} \"\n",
        "                    f\"iter {it_in_epoch}/{len(train_loader)} \"\n",
        "                    f\"| global {global_step}/{total_iters} \"\n",
        "                    f\"({progress*100:5.1f}%) \"\n",
        "                    f\"| loss {loss.item():.4f} \"\n",
        "                    f\"| elapsed {format_time(elapsed)} \"\n",
        "                    f\"| ETA {format_time(eta)}\"\n",
        "                )\n",
        "\n",
        "    torch.save(\n",
        "        {\n",
        "            \"student\": ema.state_dict(),  # EMAÎ•º ÏµúÏ¢Ö studentÎ°ú ÏÇ¨Ïö©\n",
        "            \"num_train_timesteps\": T,\n",
        "        },\n",
        "        save_path,\n",
        "    )\n",
        "    print(f\"=> saved EMA student to {save_path}\")\n",
        "    return ema, alphas_cumprod, T\n"
      ],
      "metadata": {
        "id": "wWkCWN18uQSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Student sample & FID evaluation**"
      ],
      "metadata": {
        "id": "x1ttLLZ9vlLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# ===== Student Distillation & FID ÏòàÏãú =====\n",
        "\n",
        "data_root = \"/content/\"  # TODO: Your dataset directory\n",
        "EPOCHS_OFFICIAL = 3\n",
        "BATCH_SIZE_OFFICIAL = 128\n",
        "LEARNING_RATE_OFFICIAL = 2e-4\n",
        "\n",
        "# Student distillation ÏàòÌñâ\n",
        "student_ema, student_alphas_cumprod, T = train_student_ddpm_distill(\n",
        "    data_root=data_root,\n",
        "    teacher_ckpt=\"/content/teacher.pt\",\n",
        "    epochs=EPOCHS_OFFICIAL,\n",
        "    batch_size=BATCH_SIZE_OFFICIAL,\n",
        "    lr=LEARNING_RATE_OFFICIAL,\n",
        "    save_path=\"student_ddpm_tiny.pt\",\n",
        ")\n",
        "\n",
        "def compute_fid_with_pytorch_fid(fake_dir: str, real_dir: str):\n",
        "    \"\"\"\n",
        "    pytorch-fidÎ•º subprocessÎ°ú Ìò∏Ï∂úÌï¥ÏÑú FID Í∞íÏùÑ floatÎ°ú Î∞òÌôò.\n",
        "    pip install pytorch-fid ÌïÑÏöî.\n",
        "    \"\"\"\n",
        "    cmd = [sys.executable, \"-m\", \"pytorch_fid\", fake_dir, real_dir]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    res = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    if res.returncode != 0:\n",
        "        print(\"stderr:\", res.stderr)\n",
        "        raise RuntimeError(\"FID computation failed\")\n",
        "\n",
        "    fid_val = None\n",
        "    for line in res.stdout.splitlines():\n",
        "        if \"FID:\" in line:\n",
        "            try:\n",
        "                fid_val = float(line.strip().split(\"FID:\")[-1])\n",
        "            except Exception:\n",
        "                pass\n",
        "    print(res.stdout)\n",
        "    return fid_val\n",
        "\n",
        "# 2) Student ÏÉòÌîå ÏÉùÏÑ± (Ïòà: 10,000Ïû•)\n",
        "@torch.no_grad()\n",
        "def generate_student_samples_for_fid(\n",
        "    student_model,\n",
        "    alphas_cumprod,\n",
        "    num_train_timesteps,\n",
        "    out_dir: str = \"./student_samples_baseline\",\n",
        "    n_samples: int = 10000,\n",
        "    batch_size: int = 64,\n",
        "    steps: int = 10,\n",
        "    log_interval_sec: float = 60.0,  # ~1Î∂ÑÎßàÎã§ ÏßÑÌñâ Î°úÍ∑∏\n",
        "):\n",
        "    \"\"\"\n",
        "    StudentÏö© ÏÉòÌîå ÏÉùÏÑ± + latency Ï∏°Ï†ï Ìï®Ïàò (FID ÌèâÍ∞ÄÏö©).\n",
        "\n",
        "    Î∞òÌôò:\n",
        "      - out_dir: Ïù¥ÎØ∏ÏßÄÍ∞Ä Ï†ÄÏû•Îêú ÎîîÎ†âÌÜ†Î¶¨\n",
        "      - latency_ms: Ïù¥ÎØ∏ÏßÄ 1Ïû•Îãπ ÌèâÍ∑† ÏãúÍ∞Ñ (ms)\n",
        "    \"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    student_model.eval()\n",
        "\n",
        "    left = n_samples\n",
        "    idx = 0  # ÏßÄÍ∏àÍπåÏßÄ ÏÉùÏÑ±Îêú Ïù¥ÎØ∏ÏßÄ Ïàò\n",
        "\n",
        "    start_time = time.time()\n",
        "    last_log_time = start_time\n",
        "\n",
        "    print(\n",
        "        f\"[student sample] start | n_samples={n_samples}, \"\n",
        "        f\"batch_size={batch_size}, steps={steps}\"\n",
        "    )\n",
        "\n",
        "    while left > 0:\n",
        "        bs = min(batch_size, left)\n",
        "        z = torch.randn(bs, 3, 64, 64, device=device)\n",
        "\n",
        "        # DDIM ÏÉòÌîåÎßÅ\n",
        "        x = ddim_sample(\n",
        "            student_model,\n",
        "            alphas_cumprod,\n",
        "            num_train_timesteps,\n",
        "            z,\n",
        "            steps=steps,\n",
        "        )\n",
        "        x = (x.clamp(-1, 1) * 0.5 + 0.5)  # [-1,1] -> [0,1]\n",
        "\n",
        "        # Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
        "        for i in range(bs):\n",
        "            save_image(x[i], os.path.join(out_dir, f\"s{idx:06d}.png\"))\n",
        "            idx += 1\n",
        "        left -= bs\n",
        "\n",
        "        # ----- ÏßÑÌñâ Î°úÍ∑∏: 1Î∂ÑÎßàÎã§ + ÎßàÏßÄÎßâÏóê Ìïú Î≤à -----\n",
        "        now = time.time()\n",
        "        if (now - last_log_time) >= log_interval_sec or idx == n_samples:\n",
        "            elapsed = now - start_time\n",
        "            progress = idx / n_samples\n",
        "            eta = elapsed / progress - elapsed if progress > 0 else 0.0\n",
        "\n",
        "            print(\n",
        "                f\"[student sample] {idx}/{n_samples} \"\n",
        "                f\"({progress*100:5.1f}%) | \"\n",
        "                f\"elapsed {format_time(elapsed)} | \"\n",
        "                f\"ETA {format_time(eta)}\"\n",
        "            )\n",
        "            last_log_time = now\n",
        "        # ------------------------------------------\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    latency_ms = total_time / n_samples * 1000.0\n",
        "\n",
        "    print(\n",
        "        f\"=> generated {n_samples} student samples in \"\n",
        "        f\"{format_time(total_time)} \"\n",
        "        f\"({latency_ms:.2f} ms / image)\"\n",
        "    )\n",
        "\n",
        "    return out_dir, latency_ms\n",
        "\n",
        "def log_result_to_csv(\n",
        "    csv_path: str,\n",
        "    run_id: str,\n",
        "    fid: float,\n",
        "    latency_ms: float,\n",
        "):\n",
        "    \"\"\"\n",
        "    Í≤∞Í≥ºÎ•º CSVÎ°ú Í∏∞Î°ù.\n",
        "    - csv_pathÍ∞Ä ÏóÜÏúºÎ©¥ Ìó§Îçî(id,fid,latency_ms)Î•º ÎßåÎì§Í≥†,\n",
        "    - ÏûàÏúºÎ©¥ Îß® Îí§Ïóê Ìïú Ï§Ñ append.\n",
        "    \"\"\"\n",
        "    file_exists = os.path.isfile(csv_path)\n",
        "\n",
        "    with open(csv_path, \"a\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"id\", \"fid\", \"latency_ms\"])\n",
        "        writer.writerow([run_id, fid, latency_ms])\n",
        "\n",
        "    print(f\"[csv] saved result to {csv_path} (id={run_id})\")\n",
        "\n",
        "\n",
        "fake_dir_student, latency_ms = generate_student_samples_for_fid(\n",
        "    student_model=student_ema,\n",
        "    alphas_cumprod=student_alphas_cumprod,\n",
        "    num_train_timesteps=T,\n",
        "    out_dir=\"./student_samples_baseline\",\n",
        "    n_samples=10000,\n",
        "    batch_size=64,\n",
        "    steps=10,   # üîπ stepsÎ°ú ÎßûÏ∂îÍ∏∞\n",
        ")\n",
        "\n",
        "# real_dirÎäî ÏïûÏóêÏÑú flatten_tiny_valÎ°ú ÎßåÎì† \"./tiny_val_flat\" ÏÇ¨Ïö©\n",
        "fid_student = compute_fid_with_pytorch_fid(fake_dir=fake_dir_student, real_dir=\"./tiny_val_flat\")\n",
        "print(\"Student FID (baseline):\", fid_student)\n",
        "\n",
        "\n",
        "log_result_to_csv(\n",
        "    csv_path=\"submission.csv\",\n",
        "    run_id=0,\n",
        "    fid=fid_student,\n",
        "    latency_ms=latency_ms,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGO39nY_uQQR",
        "outputId": "c27c3508-66ff-4d2c-8f16-f778f26bd4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] train_dir: /content/tiny-imagenet-200/train\n",
            "[info] Tiny-ImageNet train size: 100000\n",
            "[student distill] start | epochs=3, total_iters‚âà2343\n",
            "[student distill] epoch 1/3 iter 1/781 | global 1/2343 (  0.0%) | loss 1.0586 | elapsed 00:00:04 | ETA 03:13:46\n",
            "[student distill] epoch 1/3 iter 50/781 | global 50/2343 (  2.1%) | loss 0.0777 | elapsed 00:02:10 | ETA 01:40:06\n",
            "[student distill] epoch 1/3 iter 100/781 | global 100/2343 (  4.3%) | loss 0.0614 | elapsed 00:04:23 | ETA 01:38:36\n",
            "[student distill] epoch 1/3 iter 150/781 | global 150/2343 (  6.4%) | loss 0.0400 | elapsed 00:06:36 | ETA 01:36:37\n",
            "[student distill] epoch 1/3 iter 200/781 | global 200/2343 (  8.5%) | loss 0.0378 | elapsed 00:08:49 | ETA 01:34:32\n",
            "[student distill] epoch 1/3 iter 250/781 | global 250/2343 ( 10.7%) | loss 0.0285 | elapsed 00:11:02 | ETA 01:32:26\n",
            "[student distill] epoch 1/3 iter 300/781 | global 300/2343 ( 12.8%) | loss 0.0203 | elapsed 00:13:15 | ETA 01:30:15\n",
            "[student distill] epoch 1/3 iter 350/781 | global 350/2343 ( 14.9%) | loss 0.0245 | elapsed 00:15:28 | ETA 01:28:05\n",
            "[student distill] epoch 1/3 iter 400/781 | global 400/2343 ( 17.1%) | loss 0.0240 | elapsed 00:17:41 | ETA 01:25:54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nujOhcuYuQJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ds1UNcSXuQHE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}